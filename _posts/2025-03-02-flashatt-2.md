## Flash attention 2

### Motivation

- For long sequence, improve parallelization
- MatMul operations are much faster -> reduce non-matmul operations

### Key methodology

#### Algorithm optimizations

##### Forward pass

1. Delayed scaling

    <U>In Flash Attention 1</u>:

    ![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfvkfYcs-bZTVNwVdMf6_R7mvhGWpZbRJkcVMSRqgUFPUW012c2u_mdT91YzsgAc0o1I9GxyMCBd8Hi2Ck9rHs5gCbYyWjimBSm9zbsEBL5OpNk4w7p5KA6_QiSVlg1SGj8reOedA?key=mO45RDahKXGVVo7FNz3JLKpP)

    The two diag(_) operations are for making sure that, at any moment Oi makes sense for being partial of its final results;

    But actually we can delay the external diag() operation to only make sure that the eventual result is correct

    <u>In flash attention 2</u>:

    ![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcqzCoGbDAWR5TdOqaWJPvz5u81P1Gz5_blTan551kIVbbjskoIXRJLl-7BHZwHCygq43NVIPlRg7Osud-WVsM28_VMRUg9-e2eub5qWSCfKPGUONPVaTN5jGpPyjKd-5ZdhLiQgg?key=mO45RDahKXGVVo7FNz3JLKpP)

    Compared to FlashAtt-1, FlashAtt-2’s flipping of the outer and inner loop is for parallelism (see later)

2. Causal masking

    Causal masking: for S = QK<sup>T</sup> at S<sub>ij</sub> with j > i

    With the chunking of flash attention,

    Some of the blocks certainly have ∀j > ∀i .

    -> only need to apply causal mask to 1 block in a row of chunks ()

3. Logsumexp

    For the backward pass, we don’t use the mi and li statistics anymore, instead we use,

    ```
    L(j) = m(j) + log(l(j))
    ```

    (to reduce two matrices to one)

##### Backward pass

Not really changed, just using L(j) instead of m and l (for recomputation of P)

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdiC9PFzEIOoeBoMW9M7CTo5t1sgpEXasV80RZVicaY-cjKOrHzoTaQi1wUT7Hvjkr92Bn3eUO8VB0WsP6iShZ9WyhClWCAaR4RD2z-u0FzPBKSKCe_ClTYTkKQeNy9DLd-HjO59Q?key=mO45RDahKXGVVo7FNz3JLKpP)

#### Improved Parallelization

<u>In Flash Attention 1</u>:

One thread block for each head, on each SM;

We actually parallel over K and V this way (K and V are in outer loop),

But when d << N this would be awkward, since we don’t have enough blocks to fill in all SMs, therefore harming GPU utilization;

<u>In Flash Attention 2</u>:

Several thread blocks for each head;

- In forward pass, each SM takes care of one block of rows;

    (parallelization over sequence length)

    Each “worker”(SM) takes one block of Q, 

    no shared computation between different workers;

    -> improves GPU utilization

- In backward pass, still, parallelize over sequence length,

    each SM takes care of one block of columns;

    The only shared computation between different workers:

    ![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXfZGSsMUQ-q4--fW0nBP9LSzGg9fHP1cP6CNzptpRLBB45CAVoYiseqiVWBJ3FGjD6GQhZOi47lPr-tgIkB0JXt82C4wqKZiTLt8XBFD0DVeRxTci7i8EFVnhph9NV64VUyn6GX?key=mO45RDahKXGVVo7FNz3JLKpP)

    Use **atomic adds** to resolve this:

    ```cpp

    __global__ void atomic_add (float *dQ, float *dS, float *K, int index) {

        float update = dS[index] * K[index];

        atomicAdd(&dQ[index], update);

    }

    ```

- What if we parallelize by row in backward pass?

    Race condition here:

    ![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXd53ZB61SBVhCJpaPoCxy_8d9q7yjre29zfxLK26BE5ioLWZq82uK72f9IKVJ_Xd1b1XkMx9IweGfEA_nXGsbE4rbSumR4Bzut7w-OhNDGadMi4EmexjemP3vIldmqWA3iO1zl0fA?key=mO45RDahKXGVVo7FNz3JLKpP)

    ![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcVq7xPPsoqwH4y7D-N07R576GRezmKCgDKF5IC5xQ5MyPqYNwqqadrzuyhc-upS8vYyAN9i4_RlGp2x_kJkCgYpj5_G5dypZYBPvJM4J-x8kfMjY7MqDh8elJMyA_MyJW3NOgU?key=mO45RDahKXGVVo7FNz3JLKpP)

#### Work Partitioning (among warps)

##### What is a warp

- A thread block (SM) consists of multiple warps, usually 32 threads / warp for nvidia gpus
- **SIMT (Single Instruction, Multiple Threads)**
- Not directly programmable

##### Warps & registers

- Warps communicate with each other via shared memory, whereas threads within a warp communicate with each other via registers (much faster);
- Each thread has its own private registers, SMs have a shared register pool

##### Flash-attention 2 work partitioning

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcNLT63eSDrxPv7y895Xymg8iCr5d297Gxe19IYZFpoNGbvvOLMdhX3nFjvz5btZQxoSb_DzeLzTwiiCm-O_aixzab7TzdsHdiF-4ywcaCdEPtbFItuI9gIHr5lb28zmpRANlUDpQ?key=mO45RDahKXGVVo7FNz3JLKpP)

- What’s wrong with flash-attention 1

    ![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcu0PGay3shZZUeCQTkEtjW7x4vvbgovkQRZt0T5HB572Mg0CwM05lVMWxWeJKFJd9SWd7z_p1qXt4ZLyhqLaixEieiC7FEjLDqsxAOswz4iBR6ywurGA99NK_yKqQ8EPoUO1gjfQ?key=mO45RDahKXGVVo7FNz3JLKpP)

    Within a thread block, we split Q among warps,

    Different warps process different columns of O<sub>i</sub><sup>(j)</sup> independently,

    Thus each warp gets a slice of QK<sup>T</sup>

    Therefore all warps need to write their intermediate results out to shared memory, synchronize, then add up the intermediate results.

- What’s good in flash-attention 2

    ![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXdbFfoBTrjjXah5PmLAefiqM78Ga9EMIlk8PZsRchvgGDYnjOY9AqDroNuuLg_zTOzuaQs_lEIfj7VNJvEOn0GWPsef-Itw_lA_x5nqDKLLKMqSm4rBGO9Mn13sfcFwCGNt6pJFnw?key=mO45RDahKXGVVo7FNz3JLKpP)

    Now Q is shared across warps, instead K and V are split;

    Different warps process different rows of Oi(j) independently,

    No warp-to-warp communication anymore.